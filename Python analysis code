#Importing Libraries
import pandas as pd
import numpy as np
from faker import Faker
import random
from geopy.geocoders import Nominatim
from datetime import datetime, timedelta
from sklearn.metrics import silhouette_score, mean_absolute_error, r2_score, davies_bouldin_score
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import geopandas as gpd
import folium
from folium.plugins import MarkerCluster

#To generate fake data
fake = Faker()
fake_uk = Faker('en_GB') #select UK as location

#Function to generate unique 5 digit customer ID
def generate_unique_customer_id(existing_ids):
    while True:
        customer_id = fake.random_int(min=10000, max=99999)
        if customer_id not in existing_ids:
            return customer_id

# Function to generate the customer data
def generate_customer_data(num_customers):
    customer_data = []
    customer_ids = []

    age_ranges = [
        (18, 24, 0.16),
        (25, 34, 0.26),
        (35, 44, 0.23),
        (45, 54, 0.18),
        (55, 64, 0.10),
        (65, 100, 0.07)
    ]

    def select_age():
        ranges, probabilities = zip(*[(range(min_age, max_age + 1), prob) for min_age, max_age, prob in age_ranges])
        selected_range = random.choices(ranges, probabilities, k=1)[0]
        return random.choice(selected_range)
    
    gender_distribution = {
        'Male': 0.531,
        'Female': 0.469
    }
    
    def select_gender():
        return random.choices(list(gender_distribution.keys()), list(gender_distribution.values()), k=1)[0]
    
    def generate_date_last_15_years():
        end_date = datetime.today()
        start_date = end_date - timedelta(days=15*365)
        return fake.date_between(start_date=start_date, end_date=end_date)

    for _ in range(num_customers):
        customer_id = generate_unique_customer_id(customer_ids)
        customer_ids.append(customer_id)
        location = fake_uk.city()
        latitude, longitude = fake_uk.local_latlng(country_code="GB", coords_only=True)
        customer_data.append([
            customer_id,
            fake.name(),
            fake.email(),
            generate_date_last_15_years(),
            latitude,
            longitude,
            select_age(),
            select_gender()
        ])

    return customer_data, customer_ids

#Generate data
num_customers = 1000
customer_data, customer_ids = generate_customer_data(num_customers)
customer_df = pd.DataFrame(customer_data, columns=['customer_id', 'name', 'email', 'join_date', 'latitude', 'longitude', 'age', 'gender'])
customer_df

#Function to generate transactions using customer_id from customer_data
def generate_transaction_data_fixed(num_transactions, customer_ids):
    transaction_data = []
    
    #go through each customer_id and generate multiple transactions for each customer
    for i in range(num_transactions):
        #randomly select a customer_id from the customer_id list
        customer_id = random.choice(customer_ids)  
        amount = round(random.uniform(10, 1000), 2)
        
        #Introducing outliers
        if random.random() < 0.05:  # 5% outliers
            amount *= random.uniform(2, 5)  
        
        transaction_data.append([
            fake.uuid4(),# transaction_id
            customer_id,  
            fake.date_this_year(),
            amount,  
            random.choice(['Electronics', 'Clothing', 'Food', 'Books']),
            random.choice(['Credit Card', 'Debit Card', 'Paypal']),
            random.randint(1, 10),  # quantity
            round(random.uniform(1, 100), 2),  # price
            random.choice([0, 5, 10, 15]),  # discount
            random.choice(['Completed', 'Pending', 'Failed']),
            random.randint(1, 5)  # review rating
        ])
    
    return transaction_data

# Generate 5000 transactions
num_transactions = 5000  
transaction_data = generate_transaction_data_fixed(num_transactions, customer_ids)

#transaction data
transaction_df = pd.DataFrame(transaction_data, columns=[
    'transaction_id', 'customer_id', 'transaction_date', 'amount', 
    'product_category', 'payment_method', 'quantity', 
    'unit_price', 'discount', 'transaction_status', 'review_rating'
])
transaction_df

#Function to generate behavioral data using customer ID from customer_data
def generate_behavioral_data(customer_ids):
    behavioral_data = []
    for customer_id in customer_ids:
        last_login_date = fake.date_between(start_date='-2M', end_date='today')
        behavioral_data.append([
            customer_id,
            last_login_date,
            random.randint(1, 100),
            random.randint(1, 50),
            round(random.uniform(1, 120)),
            random.randint(0, 20),
            random.choice(['Search Engine', 'Social Media', 'Direct', 'Email']),
            random.choice(['Mobile', 'Desktop', 'Tablet']),
            round(random.uniform(10, 500), 2),
            random.randint(0, 50),
            round(random.uniform(1, 5)),
            random.randint(0, 10)
        ])
    return behavioral_data


#Check number of rows
print(transaction_df.shape)

#Generate behavioral data 
behavioral_data = generate_behavioral_data(customer_ids)
behavioral_df = pd.DataFrame(behavioral_data, columns=['customer_id', 'last_login_date', 'total_login_count', 'pages_visited', 'time_spent_on_site', 'num_transactions', 'referral_source', 'device_used', 'average_order_value', 'promo_clicks', 'sessions_per_day', 'social_shares'])
behavioral_df

#Get the unique customer_id from each dataframe
customer_ids_customers = set(customer_df['customer_id'])
customer_ids_transactions = set(transaction_df['customer_id'])
customer_ids_behavioral = set(behavioral_df['customer_id'])

# common customer_id between customers and transactions
common_customer_ids_customers_transactions = customer_ids_customers.intersection(customer_ids_transactions)
common_customer_ids_customers_transactions_count = len(common_customer_ids_customers_transactions)

# common customer_id between customers and behavioral
common_customer_ids_customers_behavioral = customer_ids_customers.intersection(customer_ids_behavioral)
common_customer_ids_customers_behavioral_count = len(common_customer_ids_customers_behavioral)

#results
print(f"Number of matching customer_ids between customers and transactions: {common_customer_ids_customers_transactions_count} out of {len(customer_ids_customers)}")
print(f"Number of matching customer_ids between customers and behavioral: {common_customer_ids_customers_behavioral_count} out of {len(customer_ids_customers)}")


#save the dataframes
customer_df.to_csv('customers.csv', index=False)
transaction_df.to_csv('transactions.csv', index=False)
behavioral_df.to_csv('behavioral.csv', index=False)



#RFM Analysis

#calculate RFM scores using transaction_df
transaction_df['transaction_date'] = pd.to_datetime(transaction_df['transaction_date'])
latest_date = transaction_df['transaction_date'].max()

rfm_corrected = transaction_df.groupby('customer_id').agg({
    'transaction_date': lambda x: (latest_date - x.max()).days,
    'transaction_id': 'count',
    'amount': 'sum'
}).rename(columns={'transaction_date': 'Recency', 'transaction_id': 'Frequency', 'amount': 'Monetary'})



#Normalize the RFM values 
rfm_to_scale = rfm_corrected[['Recency', 'Frequency', 'Monetary']]
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm_to_scale)


#create a new df for normalized values
rfm_normalized = pd.DataFrame(rfm_scaled, index=rfm_corrected.index, columns=['Recency_Normalized', 'Frequency_Normalized', 'Monetary_Normalized'])
print(rfm_normalized.head())

#combine normalized df back to original RFM df
rfm_final = pd.concat([rfm_corrected, rfm_normalized], axis=1)

# Final df
print(rfm_final.head())

def rfm_score(df, column, bins, ascending=True):
    #determine the number of unique quantiles in the column
    unique_values = df[column].nunique()
    
    #if there are fewer unique values than bins, reduce the number of bins
    if unique_values < bins:
        bins = unique_values
    
    #define the labels based on the adjusted number of bins
    labels = range(1, bins + 1) if ascending else range(bins, 0, -1)
    
    return pd.qcut(df[column], q=bins, labels=labels, duplicates='drop')

#re-apply the scoring
rfm_corrected['R_Score'] = rfm_score(rfm_corrected, 'Recency', 5, ascending=False)
rfm_corrected['F_Score'] = rfm_score(rfm_corrected, 'Frequency', 5)
rfm_corrected['M_Score'] = rfm_score(rfm_corrected, 'Monetary', 5)

#combine R, F, M scores to create an RFM score
rfm_corrected['RFM_Score'] = (
    rfm_corrected['R_Score'].astype(str) +
    rfm_corrected['F_Score'].astype(str) +
    rfm_corrected['M_Score'].astype(str)
)

print("RFM Scores DataFrame:")
print(rfm_corrected[['Recency', 'Frequency', 'Monetary', 'R_Score', 'F_Score', 'M_Score', 'RFM_Score']])

print("RFM Scores DataFrame:")
print(rfm_corrected[['Recency', 'Frequency', 'Monetary', 'R_Score', 'F_Score', 'M_Score', 'RFM_Score']])

#Conditions for the segmentation
def dynamic_segment_customer(df):
    if df['R_Score'] >= 4 and df['F_Score'] >= 4 and df['M_Score'] >= 4:
        return 'VIP'  # high recency, frequency, and monetary
    elif df['R_Score'] >= 4 and df['F_Score'] >= 3 and df['M_Score'] >= 3:
        return 'Loyal'  # high recency, moderate frequency, and moderate monetary
    elif df['R_Score'] >= 4 and df['F_Score'] >= 3:
        return 'Recent'  # high recency and moderate frequency, regardless of spending
    elif df['M_Score'] >= 4 and df['F_Score'] >= 3:
        return 'Big Spender'  # High monetary and moderate frequency
    elif df['R_Score'] <= 2 and df['F_Score'] <= 2 and df['M_Score'] <= 2:
        return 'Lost'  # low recency, frequency, and monetary
    elif df['R_Score'] <= 2 and df['F_Score'] >= 3:
        return 'At Risk'  # low recency but moderate frequency
    elif df['R_Score'] <= 2 and df['F_Score'] <= 2:
        return 'About to Sleep'  # low Recency and low frequency
    elif df['M_Score'] <= 2 and df['F_Score'] <= 2:
        return 'Low Spender'  # low monetary and low frequency
    else:
        return 'Others'  #any other conditions


#apply the segmentation function to the rfm_corrected DataFrame
rfm_corrected['Segment'] = rfm_corrected.apply(dynamic_segment_customer, axis=1)

#display the segmented df
print(rfm_corrected[['RFM_Score', 'Segment']].head())

#reset the index of the RFM df to move 'customer_id' from index to a column
rfm = rfm_corrected.reset_index()
print(rfm.head())

# merge to join the 'segment' from rfm to customers
customer_df = customer_df.merge(rfm[['customer_id', 'Segment']], on='customer_id', how='left')


#Check results
print(customer_df.head())

#Count the number of customers in each segment
segment_counts = rfm['Segment'].value_counts()

print("Customer count by segment:\n", segment_counts)

segment_rfm_means = rfm.groupby('Segment').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': 'mean'
})

print("\nMean RFM scores by segment:\n", segment_rfm_means)

#calculate the total monetary value for each segment
segment_monetary_totals = rfm.groupby('Segment')['Monetary'].sum()

print("\nTotal Monetary value by segment:\n", segment_monetary_totals)


# Plot the number of customers in each segment
plt.figure(figsize=(10, 6))
segment_counts.plot(kind='bar', color='skyblue')
plt.title('Number of Customers in Each Segment')
plt.xlabel('Segment')
plt.ylabel('Number of Customers')
plt.show()



# Plot boxplots of RFM scores by segment
plt.figure(figsize=(15, 8))
sns.boxplot(x='Segment', y='Recency', data=rfm)
plt.title('Recency Score Distribution by Segment')
plt.show()

plt.figure(figsize=(15, 8))
sns.boxplot(x='Segment', y='Frequency', data=rfm)
plt.title('Frequency Score Distribution by Segment')
plt.show()

plt.figure(figsize=(15, 8))
sns.boxplot(x='Segment', y='Monetary', data=rfm)
plt.title('Monetary Score Distribution by Segment')
plt.show()


#create cohort labels by binning Recency into categories directly
rfm_corrected['Cohort'] = pd.cut(
    rfm_corrected['Recency'], 
    bins=[0, 30, 90, 180, 365, 1000], 
    labels=['<30 Days', '30-90 Days', '90-180 Days', '180-365 Days', '>365 Days']
)

#check cohort distribution
print(rfm_corrected[['Recency', 'Cohort']].head())

#Group by Cohort and segment to get the customer count for each group
cohort_counts = rfm_corrected.groupby(['Cohort', 'Segment']).size().unstack(fill_value=0)

#visualize the customer distribution across cohorts and segments
plt.figure(figsize=(10, 6))
sns.heatmap(cohort_counts, cmap="YlGnBu", annot=True, fmt='d')
plt.title('Customer Distribution by Cohort and Segment')
plt.ylabel('Cohort')
plt.xlabel('Segment')
plt.show()

import matplotlib.pyplot as plt

# histogram for Frequency
plt.figure(figsize=(10, 6))
plt.hist(rfm['Frequency'], bins=20, color='lightgreen', edgecolor='black')
plt.title('Distribution of Frequency Scores')
plt.xlabel('Frequency (number of purchases)')
plt.ylabel('Number of Customers')
plt.show()

# histogram for Monetary
plt.figure(figsize=(10, 6))
plt.hist(rfm['Monetary'], bins=20, color='salmon', edgecolor='black')
plt.title('Distribution of Monetary Scores')
plt.xlabel('Monetary (total amount spent)')
plt.ylabel('Number of Customers')
plt.show()





# merge RFM and Behavioral Dataframes
rfm_behavioral = pd.merge(rfm, behavioral_df, on='customer_id')


#Select features for clustering
features = rfm_behavioral[['Recency', 'Frequency', 'Monetary', 
                           'total_login_count', 'pages_visited', 
                           'time_spent_on_site', 'num_transactions', 
                           'promo_clicks', 'sessions_per_day', 'social_shares']]


#calculate loyalty score
rfm_behavioral['Loyalty_Score'] = (
    rfm_behavioral['total_login_count'] * 0.2 + 
    rfm_behavioral['pages_visited'] * 0.3 + 
    rfm_behavioral['time_spent_on_site'] * 0.5
)
print("Loyalty Score:")
print(rfm_behavioral[['customer_id', 'Loyalty_Score']].head())

#calculate Churn Risk score
rfm_behavioral['Churn_Risk_Score'] = (
    rfm_behavioral['Recency'] * 0.5 +  #larger recency indicates less recent activity
    (1 / rfm_behavioral['Frequency']) * 0.3 +  #inverse of frequency, fewer purchases indicate churn risk
    (1 / rfm_behavioral['total_login_count']) * 0.2  #fewer logins could indicate churn risk
)
print("\nChurn Risk Score:")
print(rfm_behavioral[['customer_id', 'Churn_Risk_Score']].head())

#calculate monetary value score
rfm_behavioral['Monetary_Value_Score'] = (
    rfm_behavioral['Monetary'] * 0.7 + 
    rfm_behavioral['average_order_value'] * 0.2 + 
    rfm_behavioral['num_transactions'] * 0.1
)
print("\nMonetary Value Score:")
print(rfm_behavioral[['customer_id', 'Monetary_Value_Score']].head())

#calculate purchase consistency score
rfm_behavioral['Purchase_Consistency_Score'] = (
    rfm_behavioral['Frequency'] * 0.5 + 
    rfm_behavioral['num_transactions'] * 0.3 + 
    (1 / rfm_behavioral['Recency']) * 0.2  #more recent interactions = higher score
)
print("\nPurchase Consistency Score:")
print(rfm_behavioral[['customer_id', 'Purchase_Consistency_Score']].head())

#calculate activity score
rfm_behavioral['Activity_Score'] = (
    rfm_behavioral['total_login_count'] * 0.3 + 
    rfm_behavioral['pages_visited'] * 0.3 + 
    rfm_behavioral['num_transactions'] * 0.4
)
print("\nActivity Score:")
print(rfm_behavioral[['customer_id', 'Activity_Score']].head())





#merge RFM and Behavioral DataFrames
rfm_behavioral = pd.merge(rfm, behavioral_df, on='customer_id')


#select features for clustering
features = rfm_behavioral[['Recency', 'Frequency', 'Monetary', 
                           'total_login_count', 'pages_visited', 
                           'time_spent_on_site', 'num_transactions', 
                           'promo_clicks', 'sessions_per_day', 'social_shares']]


#Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

#apply PCA to reduce dimensionality
pca = PCA(n_components=2)  #reducing to 2 dimensions for better visualization
features_pca = pca.fit_transform(features_scaled)

#determining the optimal k for K-Means using the Elbow method
inertia = []
K = range(1, 11)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(features_pca)
    inertia.append(kmeans.inertia_)

#plot the Elbow curve
plt.figure(figsize=(8, 4))
plt.plot(K, inertia, 'bo-')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k After PCA')
plt.show()

#apply K-Means clustering on PCA reduced data
optimal_k = 5  #use optimal k from the elbow method
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
rfm_behavioral['KMeans_PCA_Cluster'] = kmeans.fit_predict(features_pca)

#visualize K-Means clusters after PCA
plt.figure(figsize=(10, 6))
sns.scatterplot(x=features_pca[:, 0], y=features_pca[:, 1], hue=rfm_behavioral['KMeans_PCA_Cluster'], palette='Set2')
plt.title('K-Means Clustering after PCA')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

#group data by cluster labels and calculate summary statistics for each cluster
cluster_analysis = rfm_behavioral.groupby('KMeans_PCA_Cluster').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': 'mean',
    'total_login_count': 'mean',
    'pages_visited': 'mean',
    'time_spent_on_site': 'mean'
})
print(cluster_analysis)

#code to Visualize Original Features by Cluster:

print(rfm_behavioral)

#visualize original features by cluster (Recency, Frequency, Monetary)
plt.figure(figsize=(14, 8))
sns.boxplot(x='KMeans_PCA_Cluster', y='Recency', data=rfm_behavioral)
plt.title('Recency by Cluster')
plt.show()


#plot frequency by cluster
plt.figure(figsize=(14, 8))
sns.boxplot(x='KMeans_PCA_Cluster', y='Frequency', data=rfm_behavioral)
plt.title('Frequency by Cluster')
plt.show()

#plot monetary by cluster
plt.figure(figsize=(14, 8))
sns.boxplot(x='KMeans_PCA_Cluster', y='Monetary', data=rfm_behavioral)
plt.title('Monetary by Cluster')
plt.show()



#Silhouette Score
silhouette_avg = silhouette_score(features_pca, rfm_behavioral['KMeans_PCA_Cluster'])
print(f"Silhouette Score: {silhouette_avg}")

# Davies-Bouldin Index
davies_bouldin_avg = davies_bouldin_score(features_pca, rfm_behavioral['KMeans_PCA_Cluster'])
print(f"Davies-Bouldin Index: {davies_bouldin_avg}")





#Print High value cutomers based on Kmeans clusters

#Ensuring that RFM_Score is treated as a string before filtering
rfm_behavioral['RFM_Score'] = rfm_behavioral['RFM_Score'].astype(str)

#criteria for high-value customers 
high_value_customers = rfm_behavioral[rfm_behavioral['RFM_Score'] == '555']  

# high-Value customer identification based on RFMScore
print("RFM_Score value counts:")
print(rfm_behavioral['RFM_Score'].value_counts())

#print the high-value customers details
print("High-Value Customers (RFM_Score '555'):")
print(high_value_customers[['customer_id', 'Monetary', 'Frequency', 'RFM_Score', 'KMeans_PCA_Cluster']])




#DBSCAN Clustering
dbscan = DBSCAN(eps=0.25, min_samples=5)  # Adjust eps and min_samples as needed
rfm_behavioral['DBSCAN_PCA_Cluster'] = dbscan.fit_predict(features_pca)

plt.figure(figsize=(12, 6))
sns.scatterplot(x=features_pca[:, 0], y=features_pca[:, 1], hue=rfm_behavioral['DBSCAN_PCA_Cluster'], palette='viridis', legend='full')
plt.title('DBSCAN Clustering after PCA')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

#calculate Silhouette Score and Davies-Bouldin Index for DBSCAN clusters (if more than one cluster exists)
if len(non_noise_clusters) > 1:
    filtered_data = features_pca[rfm_behavioral['DBSCAN_PCA_Cluster'] != -1]
    filtered_labels = rfm_behavioral['DBSCAN_PCA_Cluster'][rfm_behavioral['DBSCAN_PCA_Cluster'] != -1]
    
    silhouette_avg = silhouette_score(filtered_data, filtered_labels)
    print(f"DBSCAN Silhouette Score after PCA: {silhouette_avg:.4f}")
    
    davies_bouldin_avg = davies_bouldin_score(filtered_data, filtered_labels)
    print(f"DBSCAN Davies-Bouldin Index after PCA: {davies_bouldin_avg:.4f}")
else:
    print("Not enough clusters (or too many noise points) to calculate Silhouette Score or Davies-Bouldin Index for DBSCAN after PCA.")

#Add DBSCAN labels to high-value customers
high_value_customers = high_value_customers.copy()
high_value_customer_indices = rfm_behavioral.index.isin(high_value_customers.index)
high_value_dbscan_labels = dbscan.labels_[high_value_customer_indices]
high_value_customers['DBSCAN_Label'] = high_value_dbscan_labels

#-1 represents noise/outliers detected by DBSCAN
outliers = high_value_customers[high_value_customers['DBSCAN_Label'] == -1]
print("Outliers (Detected by DBSCAN):")
print(outliers[['customer_id', 'Monetary', 'Frequency', 'RFM_Score', 'KMeans_PCA_Cluster', 'DBSCAN_Label']])


#non outliers (not labeled as noise)
non_outliers = high_value_customers[high_value_customers['DBSCAN_Label'] != -1]
print("Non-Outliers (Detected by DBSCAN):")
print(non_outliers[['customer_id', 'Monetary', 'Frequency', 'RFM_Score', 'KMeans_PCA_Cluster', 'DBSCAN_Label']])


#visualize high Value customers with DBSCAN labels
features_scaled_high_value = features_scaled[high_value_customer_indices]
dbscan_labels_high_value = dbscan.labels_[high_value_customer_indices]

plt.figure(figsize=(10, 6))
sns.scatterplot(x=features_scaled_high_value[:, 0], y=features_scaled_high_value[:, 1], hue=dbscan_labels_high_value, palette='coolwarm')
plt.title('DBSCAN Outlier Detection in High-Value Customers')
plt.xlabel('Monetary (Scaled)')
plt.ylabel('Frequency (Scaled)')
plt.show()



#visualize high value customers in K-Means clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x=features_pca[:, 0], y=features_pca[:, 1], hue=rfm_behavioral['KMeans_PCA_Cluster'], palette='Set2')
plt.title('High-Value Customers in K-Means Clusters')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')

# highlight high-value customers
for idx, row in high_value_customers.iterrows():
    plt.scatter(features_pca[idx, 0], features_pca[idx, 1], color='red')
plt.show()

#identify noisy customers (outliers) detected by DBSCAN
noisy_customers = rfm_behavioral[rfm_behavioral['DBSCAN_PCA_Cluster'] == -1]

#ensure that noisy_customers is properly defined before plotting
if not noisy_customers.empty:
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=features_pca[:, 0], y=features_pca[:, 1], hue=rfm_behavioral['DBSCAN_PCA_Cluster'], palette='Set1')
    plt.title('Noisy Customers Identified by DBSCAN')
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    
    # Highlight noisy customers in a different color (e.g., black)
if not noisy_customers.empty:
    for idx, row in noisy_customers.iterrows():
        plt.scatter(features_pca[idx, 0], features_pca[idx, 1], color='black')

    plt.show()
else:
    print("No noisy customers detected.")



# CLV prediction


#Prepare the Data
# CLV will be predicted based on RFM scores and behavioral metrics
features = rfm_behavioral[['Recency', 'Frequency', 
                           'total_login_count', 'pages_visited', 
                           'time_spent_on_site', 'num_transactions', 
                           'promo_clicks', 'sessions_per_day', 'social_shares']]

#the target variable (CLV) will be the 'Monetary' value. 
target = rfm_behavioral['Monetary']

#Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)


#modeling
model = LinearRegression()
model.fit(X_train, y_train)

#predict and Evaluate
y_pred = model.predict(X_test)
rfm_behavioral['CLV_Prediction'] = model.predict(features)

#display the Dataframe with CLV predictions
print(rfm_behavioral[['Recency', 'Frequency', 'Monetary', 'CLV_Prediction']].head())


#evaluate the model
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error: {mae}")
print(f"R-Squared: {r2}")




#Geospatial Analysis

#convert the Dataframe to a GeoDataFrame
gdf = gpd.GeoDataFrame(customer_df, 
                       geometry=gpd.points_from_xy(customer_df.longitude, customer_df.latitude))


#set the coordinate reference system (CRS)
gdf.set_crs(epsg=4326, inplace=True)  # WGS84 Lat/Long


#check if GeoDataFrame is correctly formed
print(gdf.head())


#filter high-value customers (VIP segment)
high_value_customers = gdf[gdf['Segment'] == 'VIP']


#convert latitude and longitude columns to numeric
customer_df['latitude'] = pd.to_numeric(customer_df['latitude'], errors='coerce')
customer_df['longitude'] = pd.to_numeric(customer_df['longitude'], errors='coerce')


#create a base map centered around the mean latitude and longitude of the customers
mean_lat = customer_df['latitude'].mean()
mean_lon = customer_df['longitude'].mean()
m = folium.Map(location=[mean_lat, mean_lon], zoom_start=6)

#define a color map for each segment
segment_color_map = {
    'VIP': 'pink',
    'Loyal': 'green',
    'At Risk': 'orange',
    'Lost': 'yellow',
    'Big Spender': 'purple',
    'Low Spender': 'blue',
    'Others': 'lightgray'  # or any other segment
}

#add customer locations to the map with unique colors for each segment
marker_cluster = MarkerCluster().add_to(m)
for idx, row in gdf.iterrows():
    # Get the color for the customer segment from the color map
    segment_color = segment_color_map.get(row['Segment'], 'lightgray')  # Default color if segment not found
    
    folium.Marker(
        location=[row['latitude'], row['longitude']],
        popup=f"Customer ID: {row['customer_id']}\nSegment: {row['Segment']}",
        icon=folium.Icon(color=segment_color)
    ).add_to(marker_cluster)

#highlight high-value customers in a different color
for idx, row in high_value_customers.iterrows():
    folium.Marker(location=[row['latitude'], row['longitude']],
                  popup=f"High-Value Customer ID: {row['customer_id']}\nSegment: {row['Segment']}",
                  icon=folium.Icon(color='red', icon='star')
                  ).add_to(m)

#save the map to an HTML file
m.save('customer_distribution_map.html')

m

#Customer Profiling and Insights

# analyze gender distribution by segment
plt.figure(figsize=(10, 6))
sns.countplot(x='Segment', hue='gender', data=customer_df)
plt.title('Gender Distribution by Segment')
plt.show()



# Purchase behavior profiling by segment
# analyzing average amount spent per segment
avg_spend_per_segment = rfm_corrected.groupby('Segment')['Monetary'].mean()
print("Average Spend per Segment:\n", avg_spend_per_segment)



# visualize average amount spent per segment
plt.figure(figsize=(10, 6))
avg_spend_per_segment.plot(kind='bar', color='coral')
plt.title('Average Spend per Segment')
plt.xlabel('Segment')
plt.ylabel('Average Monetary Value')
plt.show()



